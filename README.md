Here is your updated, polished **README.md** with the GitHub clone instruction added at the perfect place and written professionally to impress reviewers.

Iâ€™ve placed the cloning section *right before* the local setup instructions â€” the best UX for developers.

---

# ðŸŒŸ **VaaniMeter**

### *AI-Powered Spoken Introduction Evaluation Tool*

---

## ðŸ“Œ **Overview**

**VaaniMeter** is an AI-powered evaluation tool designed to score spoken self-introductions using a **strict, deterministic rubric**.
It provides instant feedback, transparent scoring, radar-chart visualization, and a downloadable professional PDF report.

The tool brings structure, clarity, and objectivity to communication assessmentâ€”making it ideal for academic evaluation, skill-building, and learning environments.

---

## ðŸš€ **Features**

### ðŸ”¹ Dual Input Support

* Paste transcript directly
* Upload `.txt` files

### ðŸ”¹ Strict Rubric-Based Scoring (0â€“100)

Scores are computed across five categories:

* **Content & Structure (40 pts)**
* **Speech Rate (10 pts)**
* **Language & Grammar (20 pts)**
* **Clarity (15 pts)**
* **Engagement (15 pts)**

### ðŸ”¹ Transparent Score Breakdown

Shows **exactly how each score was calculated**, including:

* Keyword detection
* Grammar analysis
* Filler percentages
* Sentiment score
* WPM category mapping
* Flow structure quality

### ðŸ”¹ Radar Chart Visualization

A spider chart provides a visual overview of strengths and weaknesses.

### ðŸ”¹ PDF Report Generation

Download a clean, professional report containing:

* Input transcript
* Rubric category scores
* Radar chart
* Strengths & improvement insights
* Final summary

### ðŸ”¹ JSON Output (Optional)

Raw structured data for developers or advanced reviewers.

---

## ðŸ“ **Project Structure**

| File               | Description                                                                    |
| ------------------ | ------------------------------------------------------------------------------ |
| `app.py`           | Streamlit UI: input handling, score display, chart visualizations, PDF export. |
| `scorer.py`        | Core scoring engine implementing strict rubric logic.                          |
| `utils.py`         | NLP utilities, radar chart functions, and PDF report generation tools.         |
| `requirements.txt` | Dependency list.                                                               |

---

## ðŸ§  **How the Scoring Engine Works**

### **1. Content & Structure (40 pts)**

Checks:

* Salutation quality
* Must-have keywords (*Name, Age, Class/School, Family, Hobbies*)
* Good-to-have details (fun fact, goal, strengths, achievements, etc.)
* Flow and logical order

### **2. Speech Rate (10 pts)**

Uses a fixed **52-second** duration assumption:

```
wpm = total_words / (52/60)
```

Ideal WPM: **111â€“140**

### **3. Language (20 pts)**

* Grammar: via **LanguageTool**
* Vocabulary: via **Type-Token Ratio (TTR)**

### **4. Clarity (15 pts)**

Detects filler words:
`"um", "uh", "like", "you know", "hmm", "kinda", "sort of"`
Scoring based on filler percentage.

### **5. Engagement (15 pts)**

Uses **VADER sentiment analysis** for positivity.

---

## ðŸ”„ **End-to-End Data Flow**

1. **User Input** (textarea or txt file)
2. **Processing** â†’ `app.py` sends transcript to `calculate_final_score()`
3. **Analysis** â†’ `scorer.py` performs all NLP + scoring steps
4. **Visualization** â†’ `utils.create_radar_chart()`
5. **PDF Report** â†’ `utils.create_pdf_report()`
6. **Output** â†’ scores, breakdowns, chart, and report in Streamlit

---

# ðŸ“¥ **Clone This Repository**

To get started, first clone the project:

```
git clone https://github.com/prempatel-ai/vaanimeter.git
```

---

## ðŸ›  **How to Run Locally**

1. Install dependencies:

```
pip install -r requirements.txt
```

2. Launch the app:

```
streamlit run app.py
```

3. Open in your browser:
   ðŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ðŸŒ **Deployment**

Currently not deployed.
Fully compatible with:

* Streamlit Cloud
* HuggingFace Spaces
* Render
* Railway

---

## ðŸ§ª **Testing**

Evaluated using three categories of transcripts:

* High-quality â†’ **>80**
* Average â†’ **50â€“70**
* Poor â†’ **<40**

The scoring strongly matches rubric expectations.

---

## âœ¨ **What Makes VaaniMeter Unique**

* Applies a **clear, strict rubric**, not vague NLP scoring
* Transparent and explainable output
* Professional PDF reporting
* Modern visual radar analysis
* Built for fairness and educational use

---

## âš ï¸ **Known Limitations**

* Uses fixed 52s duration for WPM â†’ may differ from real audio
* Semantic similarity intentionally removed as per task requirement
* Grammar detection may miss deep contextual issues

---

## ðŸ’™ **Acknowledgments**

Created as part of the **Nirmaan Education AI Internship Case Study**
to demonstrate product thinking, communication analysis design, and practical AI engineering through a clean, transparent scoring tool.

---

