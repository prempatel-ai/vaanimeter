 

# ðŸŒŸ **VaaniMeter**

### *AI-Powered Spoken Introduction Evaluation Tool*

---

## ðŸ“Œ **Overview**

**VaaniMeter** is an AI-powered evaluation tool designed to score spoken self-introductions using a **strict, deterministic rubric**.
It provides instant feedback, transparent scoring, radar-chart visualization, and a downloadable professional PDF report.

The tool brings structure, clarity, and objectivity to communication assessmentâ€”making it ideal for academic evaluation, skill-building, and training environments.

---

## ðŸš€ **Features**

### ðŸ”¹ Dual Input Support

* Paste transcript directly
* Upload `.txt` files

### ðŸ”¹ Strict Rubric-Based Scoring (0â€“100)

Scores are computed across five categories:

* **Content & Structure (40 pts)**
* **Speech Rate (10 pts)**
* **Language & Grammar (20 pts)**
* **Clarity (15 pts)**
* **Engagement (15 pts)**

### ðŸ”¹ Transparent Score Breakdown

Users can view **how each point was calculated**, including:

* Keyword matches
* Grammar analysis
* Filler rate
* Sentiment score
* WPM category mapping
* Flow structure rating

### ðŸ”¹ Radar Chart Visualization

A spider chart helps users visually understand their performance across categories.

### ðŸ”¹ PDF Report Generation

Download a professional report containing:

* Input transcript
* All scoring components
* Radar chart visualization
* Strengths & improvement areas
* Final score summary

### ðŸ”¹ JSON Output (Optional)

Advanced users can view raw JSON scoring data.

---

## ðŸ“ **Project Structure**

| File               | Description                                                                           |
| ------------------ | ------------------------------------------------------------------------------------- |
| `app.py`           | Main Streamlit interface: input handling, UI components, chart rendering, PDF export. |
| `scorer.py`        | Core scoring engine implementing the strict rubric evaluation logic.                  |
| `utils.py`         | Helper utilities for NLP tools, radar chart rendering, and PDF report generation.     |
| `requirements.txt` | Python dependency list.                                                               |

---

## ðŸ§  **How the Scoring Engine Works**

### **1. Content & Structure (40 pts)**

Checks:

* Quality of salutation
* Presence of must-have keywords: *Name, Age, Class/School, Family, Hobbies*
* Presence of good-to-have details: fun fact, goal, strengths, etc.
* Flow and natural ordering of introduction sections

### **2. Speech Rate (10 pts)**

Assumes a fixed **52-second** speaking duration
Computes WPM:

```
wpm = total_words / (52/60)
```

Ideal score range: **111â€“140 WPM**

### **3. Language (20 pts)**

* Grammar scoring using **LanguageTool**
* Vocabulary quality measured by **Type-Token Ratio (TTR)**

### **4. Clarity (15 pts)**

Detects filler words:
`"um, uh, like, you know, hmm, kinda, sort of"`
Score based on filler percentage.

### **5. Engagement (15 pts)**

Uses **VADER sentiment analysis** to measure positivity and overall tone.

---

## ðŸ”„ **End-to-End Data Flow**

1. **User Input**
   Text is pasted or uploaded.

2. **Processing**
   `app.py` sends transcript to `IntroductionScorer.calculate_final_score()`.

3. **Analysis**
   `scorer.py`:

   * cleans text
   * runs NLP tools
   * computes all category scores
   * generates JSON output

4. **Visualization**
   `create_radar_chart()` renders a spider chart.

5. **Report Generation**
   `create_pdf_report()` generates a downloadable report using ReportLab.

6. **Output**
   Streamlit displays all scores, breakdowns, and visualizations.

---

## ðŸ›  **How to Run Locally**

1. Install dependencies:

```
pip install -r requirements.txt
```

2. Launch the app:

```
streamlit run app.py
```

3. Open in your browser:
   ðŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ðŸŒ **Deployment**

Not yet deployed.
The project is fully compatible with:

* Streamlit Cloud
* HuggingFace Spaces
* Render
* Railway

---

## ðŸ§ª **Testing**

Testing was done on:

* High-quality transcripts (expected: **>80**)
* Average transcripts (**50â€“70**)
* Poor transcripts (**<40**)

Old testing scripts are removed after cleanup.
The final scoring matches rubric expectations consistently.

---

## âœ¨ **What Makes VaaniMeter Unique**

* Uses a **strict professional rubric**, not generic NLP heuristics
* Transparent and explainable scoring
* Professional-grade PDF reporting
* Visual feedback through radar chart
* Designed with clarity, fairness, and educational use-cases in mind

---

## âš ï¸ **Known Limitations**

* Assumes fixed 52-second audio duration; actual speaking speed may differ
* Semantic similarity has been intentionally removed per task requirement
* Grammar scoring relies on automated tools which may not catch all context-based errors

---

## ðŸ’™ **Acknowledgments**

This project was created as part of the Nirmaan Education AI Internship Case Study
â€” to demonstrate product thinking, communication assessment design, and practical AI engineering.

---

